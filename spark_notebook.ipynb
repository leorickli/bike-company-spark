{"cells": [{"cell_type": "markdown", "id": "9e6c91fd-8d61-40c4-a979-779d1c185cb3", "metadata": {}, "source": "# Spark Notebook\n\nThis file contains the dataset from the [Hadoop-Hive](https://github.com/leorickli/bike-company-hadoop/blob/main/README.md) project. There will be some data analysis and machine learning using PySpark."}, {"cell_type": "code", "execution_count": 84, "id": "2dc17aac-e736-419e-ad23-abd787544a01", "metadata": {}, "outputs": [], "source": "from pyspark.sql.functions import desc, round, col, concat, to_date, count, avg"}, {"cell_type": "code", "execution_count": 85, "id": "3b1390a9-3046-4f4d-bd1b-0b16d7c7ff6c", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "Customer = spark.read.csv(\"gs://infnet-project-leo/Customer.csv\", header=True, inferSchema=True)\nPerson = spark.read.csv(\"gs://infnet-project-leo/Person.csv\", header=True, inferSchema=True)\nProduct = spark.read.csv(\"gs://infnet-project-leo/Product.csv\", header=True, inferSchema=True)\nSalesOrderDetail = spark.read.csv(\"gs://infnet-project-leo/SalesOrderDetail.csv\", header=True, inferSchema=True)\nSalesOrderHeader = spark.read.csv(\"gs://infnet-project-leo/SalesOrderHeader.csv\", header=True, inferSchema=True)\nSpecialOfferProduct = spark.read.csv(\"gs://infnet-project-leo/SpecialOfferProduct.csv\", header=True, inferSchema=True)\n"}, {"cell_type": "markdown", "id": "b42af348-b8c5-490b-bbcb-6b1e6878175b", "metadata": {}, "source": "## Existing Queries\n\nThese were the queries used in the [Hadoop-Hive](https://github.com/leorickli/bike-company-hadoop/blob/main/README.md) project. The SQL queries will be translated to PySpark now. "}, {"cell_type": "markdown", "id": "daa81171-8c11-4019-8c08-5239468af956", "metadata": {}, "source": "1. Find the different types of \"PersonType\" in the \"Person\" table.\n\nSELECT DISTINCT PersonType\nFROM Person;"}, {"cell_type": "code", "execution_count": null, "id": "a5f37f69-5e42-477c-b197-871b5b995027", "metadata": {}, "outputs": [], "source": "# First Query\n\n# Select distinct values of the \"PersonType\" column\nfirst_query = person.select(\"PersonType\").distinct()\n\nfirst_query.show()"}, {"cell_type": "markdown", "id": "e037766f-ddf3-484a-aebd-678d98c42293", "metadata": {}, "source": "2. Find the top ten biggest orders done so far.\n\nSELECT totaldue\nFROM salesOrderHeader\nORDER BY totaldue DESC\nLIMIT 10;"}, {"cell_type": "code", "execution_count": 19, "id": "824a7401-ab49-4f20-93e3-bc8c745fe3ac", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------+\n|rounded_totaldue|\n+----------------+\n|       187487.83|\n|       182018.63|\n|       170512.67|\n|       166537.08|\n|       165028.75|\n|       158056.54|\n|       145741.86|\n|       145454.37|\n|       142312.22|\n|       140042.12|\n+----------------+\n\n"}], "source": "# Second Query\n\n# Select the \"totaldue\" column, round it to two decimal places, order by it in descending order\nsecond_query = salesOrderHeader.select(round(\"totaldue\", 2).alias(\"rounded_totaldue\")).orderBy(desc(\"rounded_totaldue\")).limit(10)\n\nsecond_query.show()"}, {"cell_type": "markdown", "id": "32fa53af-7044-4a4e-8f85-5d3d7480c058", "metadata": {}, "source": "3. Write a query that returns the number of rows in the Sales.SalesOrderDetail table by the SalesOrderID field, provided they have at least three rows of details.\n\nSELECT SalesOrderID as id, COUNT(*) AS qtd \nFROM salesOrderDetail as sod\nGROUP BY SalesOrderID\nHAVING qtd >= 3\nORDER BY qtd DESC\nLIMIT 10;"}, {"cell_type": "code", "execution_count": 24, "id": "6ff30c0b-5030-4da0-874d-19df092a7c90", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 35:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-----+-----+\n|   id|count|\n+-----+-----+\n|51721|   72|\n|51739|   72|\n|53465|   71|\n|51160|   71|\n|47355|   68|\n|57046|   67|\n|51120|   67|\n|51090|   66|\n|55297|   66|\n|47395|   66|\n+-----+-----+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Third Query\n# The following queries are rather complex, so I had to separate them into different lines for clarity and readability\n\n# Rename the \"SalesOrderID\" column to \"id\"\nsalesOrderDetail = salesOrderDetail.withColumnRenamed(\"SalesOrderID\", \"id\")\n\n# Group by \"id\" and count the occurrences\ngrouped_salesOrderDetail = salesOrderDetail.groupBy(\"id\").count()\n\n# Filter for rows with count (qtd) greater than or equal to 3\nfiltered_salesOrderDetail = grouped_salesOrderDetail.filter(col(\"count\") >= 3)\n\n# Order by count (qtd) in descending order\nordered_salesOrderDetail = filtered_salesOrderDetail.orderBy(col(\"count\").desc())\n\n# Limit the result to the top 10 rows\nthird_query = ordered_salesOrderDetail.limit(10)\n\nthird_query.show()"}, {"cell_type": "markdown", "id": "b9cc56cc-4602-4b7f-aee1-459be7a00c04", "metadata": {}, "source": "4. Write a query linking the Person.Person, Sales.Customer, and Sales.SalesOrderHeader tables to get a list of customer names and a count of orders placed.\n\nSELECT c.CustomerID AS id, CONCAT(p.FirstName, ' ', p.LastName) AS nome, COUNT(*) AS qtd\nFROM salesOrderHeader soh\nJOIN customer c ON soh.CustomerID = c.CustomerID\nJOIN person p ON c.PersonID = p.BusinessEntityID \nGROUP BY c.CustomerID, p.FirstName, p.LastName\nORDER BY qtd DESC\nLIMIT 10;"}, {"cell_type": "code", "execution_count": 38, "id": "5638c6b3-3012-4a94-9da3-cd76b12af3b3", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-----+--------------+-----+\n|   id|          nome|count|\n+-----+--------------+-----+\n|29994|Robin McGuigan|   12|\n|30117|  Robert Vessa|   12|\n|29489| Frances Adams|   12|\n|29950|       Yale Li|   12|\n|29844|  Nancy Hirota|   12|\n|29901|    John Kelly|   12|\n|29685|    Pamela Cox|   12|\n|29810|  Jean Handley|   12|\n|30076| Diane Tibbott|   12|\n|29637|Donna Carreras|   12|\n+-----+--------------+-----+\n\n"}], "source": "# Fourth Query\n# Had to use 'concat_ws' instead of the regular 'concat'\n# The 'concat_ws' function takes a separator (in this case, a space) and a list of columns to concatenate\n\n# Perform the necessary joins with corrected column names\njoined_df = salesOrderHeader.join(customer, \"CustomerID\", \"inner\").join(person, col(\"PersonID\") == col(\"BusinessEntityID\"), \"inner\")\n\n# Select the desired columns and perform aggregations\nfrom pyspark.sql.functions import concat_ws\n\nfourth_query = joined_df.select(col(\"CustomerID\").alias(\"id\"),\n                            concat_ws(\" \", col(\"FirstName\"), col(\"LastName\")).alias(\"nome\")) \\\n                            .groupBy(\"id\", \"nome\") \\\n                            .count() \\\n                            .orderBy(col(\"count\").desc()) \\\n                            .limit(10)\n\nfourth_query.show()"}, {"cell_type": "markdown", "id": "d68d9d7e-879b-4ba7-98f0-511a5a4cf1b0", "metadata": {}, "source": "5. Write a query showing the SalesOrderID, OrderDate, and TotalDue fields from the Sales.SalesOrderHeader table. Get only the lines where the order was placed during September/2011 and the total due is above 1,000. Sort by descending total due.\n\nSELECT \nSalesOrderID as id,\nCAST(OrderDate AS DATE) AS data, \nTotalDue AS total_devido\nFROM salesOrderHeader\nWHERE OrderDate BETWEEN '2011-09-01' AND '2011-09-30' AND TotalDue > 1000\nORDER BY total_devido;"}, {"cell_type": "code", "execution_count": 44, "id": "e3c82a8b-8a84-4a3b-a4fe-d6b8c365afa9", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+----+------------+\n| id|data|total_devido|\n+---+----+------------+\n+---+----+------------+\n\n"}], "source": "# Fifth Query\n# Query returned no values, because there is no data between that time span.\n\n# Filter the DataFrame based on the conditions\nfifth_query = salesOrderHeader.select(\n    col(\"SalesOrderID\").alias(\"id\"),\n    to_date(col(\"OrderDate\"), \"yyyy-MM-dd\").alias(\"data\"),\n    col(\"TotalDue\").alias(\"total_devido\")) \\\n    .filter((col(\"data\").between(\"2011-09-01\", \"2011-09-30\")) & (col(\"total_devido\") > 1000)) \\\n    .orderBy(\"total_devido\")\n\nfifth_query.show()"}, {"cell_type": "markdown", "id": "531be953-bc4f-425c-8c23-dbb3b2f4abf9", "metadata": {}, "source": "## Extra Queries\n\nThese are extra queries for PySpark."}, {"cell_type": "markdown", "id": "7b6898b0-aea5-4b95-8454-4aca7f15ad61", "metadata": {}, "source": "1. Count the number of products by color."}, {"cell_type": "code", "execution_count": 63, "id": "617a8060-d255-412a-b73c-9fe6f3364b82", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------------+-----+\n|       color|total|\n+------------+-----+\n|        null|  248|\n|       Black|   93|\n|      Silver|   43|\n|         Red|   38|\n|      Yellow|   36|\n|        Blue|   26|\n|       Multi|    8|\n|Silver/Black|    7|\n|       White|    4|\n|        Grey|    1|\n+------------+-----+\n\n"}], "source": "# Count the number of products by color\nfirst_extra = product.groupBy(\"color\").agg(count(\"*\").alias(\"total\")).orderBy(col(\"total\").desc())\n\nfirst_extra.show()"}, {"cell_type": "markdown", "id": "416c9df3-5701-443a-9193-0be30fb7a970", "metadata": {}, "source": "2. Give me the average unit prices for every special offer ID"}, {"cell_type": "code", "execution_count": 67, "id": "eb34d43b-7b2c-4af3-81f4-79a1af2b1388", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------+------------+\n|SpecialOfferID|AvgUnitPrice|\n+--------------+------------+\n|            14|     1029.84|\n|             7|      846.85|\n|             2|      656.59|\n|             1|      461.82|\n|            13|      349.64|\n|             9|       234.9|\n|            16|       113.0|\n|             3|      105.19|\n|             4|       61.98|\n|             5|        24.3|\n|             8|       16.82|\n|            11|       15.75|\n+--------------+------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Group by \"SpecialOfferID\" and calculate the average \"UnitPrice\" for each\ngrouped_df = salesOrderDetail.groupBy(\"SpecialOfferID\").agg(avg(\"UnitPrice\").alias(\"avg_UnitPrice\"))\n\n# Select distinct \"SpecialOfferID\" values along with the rounded average \"UnitPrice\"\ndistinct_special_offer_avg_price = grouped_df.select(\n    \"SpecialOfferID\",\n    round(\"avg_UnitPrice\", 2).alias(\"AvgUnitPrice\")\n    ).distinct()\n\n# Order the result by the rounded average in descending order\nsecond_extra = distinct_special_offer_avg_price.orderBy(\"AvgUnitPrice\", ascending=False)\n\nsecond_extra.show()"}, {"cell_type": "markdown", "id": "1b4cc7e6-651d-4dad-b3d1-6040538a9d5c", "metadata": {}, "source": "## Machine Learning\n\nThis code is a Python script that demonstrates the process of building a linear regression model for predicting the total sales of the top 10 products using the PySpark library. It assumes you have a DataFrame ml_df with the required columns: \"name\" \"OrderQty\", \"UnitPrice\", and \"OrderDate\", extracted from the \"Product\", \"SalesOrderDetail\", and \"SalesOrderHeader\" tables."}, {"cell_type": "code", "execution_count": 89, "id": "14f041ba-2d2b-43c2-8838-5fd982ccf89a", "metadata": {}, "outputs": [], "source": "from pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.feature import VectorAssembler"}, {"cell_type": "code", "execution_count": 90, "id": "1dbead32-25c9-4ff8-81c8-364bfd65d949", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+--------+---------+-------------------+\n|                name|OrderQty|UnitPrice|          OrderDate|\n+--------------------+--------+---------+-------------------+\n|Mountain-100 Blac...|       1| 2024.994|2011-05-31 00:00:00|\n|Mountain-100 Blac...|       3| 2024.994|2011-05-31 00:00:00|\n|Mountain-100 Blac...|       1| 2024.994|2011-05-31 00:00:00|\n|Mountain-100 Silv...|       1| 2039.994|2011-05-31 00:00:00|\n|Mountain-100 Silv...|       1| 2039.994|2011-05-31 00:00:00|\n|Mountain-100 Silv...|       2| 2039.994|2011-05-31 00:00:00|\n|Mountain-100 Silv...|       1| 2039.994|2011-05-31 00:00:00|\n|Long-Sleeve Logo ...|       3|  28.8404|2011-05-31 00:00:00|\n|Long-Sleeve Logo ...|       1|  28.8404|2011-05-31 00:00:00|\n|Mountain Bike Soc...|       6|      5.7|2011-05-31 00:00:00|\n|        AWC Logo Cap|       2|   5.1865|2011-05-31 00:00:00|\n|Sport-100 Helmet,...|       4|  20.1865|2011-05-31 00:00:00|\n|Road-650 Red, 44 ...|       1| 419.4589|2011-05-31 00:00:00|\n|Road-450 Red, 52 ...|       1|  874.794|2011-05-31 00:00:00|\n|HL Mountain Frame...|       1|   809.76|2011-05-31 00:00:00|\n|HL Mountain Frame...|       1| 714.7043|2011-05-31 00:00:00|\n|HL Mountain Frame...|       2| 714.7043|2011-05-31 00:00:00|\n|        AWC Logo Cap|       4|   5.1865|2011-05-31 00:00:00|\n|Long-Sleeve Logo ...|       4|  28.8404|2011-05-31 00:00:00|\n|HL Mountain Frame...|       2| 722.5949|2011-05-31 00:00:00|\n+--------------------+--------+---------+-------------------+\nonly showing top 20 rows\n\n"}], "source": "# Join the tables and select the desired columns\nml_df = Product.join(SalesOrderDetail, Product.ProductID == SalesOrderDetail.ProductID, \"inner\") \\\n                  .join(SalesOrderHeader, SalesOrderDetail.SalesOrderID == SalesOrderHeader.SalesOrderID, \"inner\") \\\n                  .select(col(\"name\"), \n                          col(\"OrderQty\"), \n                          col(\"UnitPrice\"), \n                          col(\"OrderDate\"))\n\nml_df.show()"}, {"cell_type": "code", "execution_count": 91, "id": "a1b956cb-9232-46ad-84d6-f85640bb2917", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/10/13 11:08:57 WARN Instrumentation: [0da3d781] regParam is zero, which might cause numerical instability and overfitting.\n[Stage 158:============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "Root Mean Squared Error (RMSE): 950.9888478095909\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Define the feature columns and create the feature vector\nfeature_cols = [\"OrderQty\", \"UnitPrice\"]\nassembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\nml_df = assembler.transform(ml_df)\n\n# Filter for data in the top 10 products\ntop_10_products = ml_df.groupBy(\"name\").agg({\"OrderQty\": \"sum\"}).withColumnRenamed(\"sum(OrderQty)\", \"TotalSales\")\ntop_10_products = top_10_products.sort(col(\"TotalSales\"), ascending=False).limit(10)\nml_df = ml_df.join(top_10_products, on=[\"name\"], how=\"inner\")\n\n# Split the data into training and testing sets (80% training, 20% testing)\n(trainingData, testData) = ml_df.randomSplit([0.8, 0.2], seed=123)\n\n# Create a Linear Regression model\nlr = LinearRegression(featuresCol=\"features\", labelCol=\"TotalSales\")\n\n# Train the model using the training data\nlr_model = lr.fit(trainingData)\n\n# Make predictions on the testing data\npredictions = lr_model.transform(testData)\n\n# Evaluate the model's performance\nevaluator = RegressionEvaluator(labelCol=\"TotalSales\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\n"}, {"cell_type": "code", "execution_count": null, "id": "51737d4d-e9ec-4aec-ab0e-d36609650136", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}